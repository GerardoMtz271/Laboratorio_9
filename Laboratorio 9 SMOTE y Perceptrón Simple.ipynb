{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844dfe79-cd00-4062-aa73-4aa0afefb03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-Out Accuracy (Original Data): 0.7076923076923077\n",
      "Hold-Out Accuracy (After SMOTE): 0.6716417910447762\n",
      "10-Fold CV Accuracy (Original Data): 0.7285714285714285\n",
      "10-Fold CV Accuracy (After SMOTE): 0.740909090909091\n"
     ]
    }
   ],
   "source": [
    "# Parte I - Implementación de SMOTE y Clasificadores\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import load_iris, fetch_openml\n",
    "\n",
    "# Función para cargar datasets automáticamente\n",
    "def load_datasets():\n",
    "    datasets = {}\n",
    "\n",
    "    # Cargar Iris (clases Setosa y Virginica únicamente)\n",
    "    iris = load_iris()\n",
    "    iris_X = iris['data']\n",
    "    iris_y = iris['target']\n",
    "    \n",
    "    # Filtrar solo Setosa y Virginica\n",
    "    filter_indices = np.where((iris_y == 0) | (iris_y == 1))\n",
    "    datasets['Iris'] = (iris_X[filter_indices], iris_y[filter_indices])\n",
    "\n",
    "    # Cargar Glass desde OpenML\n",
    "    glass = fetch_openml(name=\"glass\", version=1, as_frame=False)\n",
    "    glass_X = glass.data\n",
    "    glass_y = glass.target\n",
    "\n",
    "    # Convertir etiquetas a valores numéricos únicos\n",
    "    unique_labels = np.unique(glass_y)\n",
    "    label_to_numeric = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    numeric_glass_y = np.array([label_to_numeric[label] for label in glass_y])\n",
    "\n",
    "    datasets['Glass'] = (glass_X, numeric_glass_y)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# SMOTE\n",
    "def smote(X, y, minority_class, k=5):\n",
    "    X_minority = X[y == minority_class]\n",
    "    n_samples, n_features = X_minority.shape\n",
    "    new_samples = []\n",
    "\n",
    "    for _ in range(len(X_minority)):\n",
    "        i = random.randint(0, n_samples - 1)\n",
    "        neighbors = [random.randint(0, n_samples - 1) for _ in range(k)]\n",
    "        neighbor = X_minority[random.choice(neighbors)]\n",
    "        \n",
    "        diff = neighbor - X_minority[i]\n",
    "        new_sample = X_minority[i] + random.random() * diff\n",
    "        new_samples.append(new_sample)\n",
    "    \n",
    "    new_samples = np.array(new_samples)\n",
    "    X_augmented = np.vstack((X, new_samples))\n",
    "    y_augmented = np.hstack((y, np.full(len(new_samples), minority_class)))\n",
    "    \n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "# Clasificador Euclidiano\n",
    "def euclidean_classifier(X_train, y_train, X_test):\n",
    "    predictions = []\n",
    "    for x in X_test:\n",
    "        distances = np.linalg.norm(X_train - x, axis=1)\n",
    "        nearest_index = np.argmin(distances)\n",
    "        predictions.append(y_train[nearest_index])\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Validación Hold-Out\n",
    "def hold_out_validation(X, y, test_ratio=0.3):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    split_point = int(n_samples * (1 - test_ratio))\n",
    "    train_indices, test_indices = indices[:split_point], indices[split_point:]\n",
    "    return X[train_indices], y[train_indices], X[test_indices], y[test_indices]\n",
    "\n",
    "# Validación Cruzada\n",
    "def k_fold_cross_validation(X, y, k=10):\n",
    "    n_samples = len(y)\n",
    "    fold_size = n_samples // k\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    folds = [indices[i * fold_size:(i + 1) * fold_size] for i in range(k)]\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(k):\n",
    "        test_indices = folds[i]\n",
    "        train_indices = np.hstack([folds[j] for j in range(k) if j != i])\n",
    "        \n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_test, y_test = X[test_indices], y[test_indices]\n",
    "        \n",
    "        predictions = euclidean_classifier(X_train, y_train, X_test)\n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "        scores.append(accuracy)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Cargar los datasets\n",
    "datasets = load_datasets()\n",
    "\n",
    "# Trabajar con el dataset Glass\n",
    "X, y = datasets['Glass']\n",
    "\n",
    "# Identificar clase minoritaria\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "minority_class = unique_classes[np.argmin(class_counts)]\n",
    "\n",
    "# Validación Hold-Out\n",
    "X_train, y_train, X_test, y_test = hold_out_validation(X, y)\n",
    "print(\"Hold-Out Accuracy (Original Data):\", np.mean(euclidean_classifier(X_train, y_train, X_test) == y_test))\n",
    "\n",
    "# Aplicar SMOTE\n",
    "X_smote, y_smote = smote(X, y, minority_class)\n",
    "\n",
    "X_train_smote, y_train_smote, X_test_smote, y_test_smote = hold_out_validation(X_smote, y_smote)\n",
    "print(\"Hold-Out Accuracy (After SMOTE):\", np.mean(euclidean_classifier(X_train_smote, y_train_smote, X_test_smote) == y_test_smote))\n",
    "\n",
    "# Validación Cruzada\n",
    "print(\"10-Fold CV Accuracy (Original Data):\", k_fold_cross_validation(X, y))\n",
    "print(\"10-Fold CV Accuracy (After SMOTE):\", k_fold_cross_validation(X_smote, y_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b669576-0d0d-4956-92e7-8ca15359b7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-Out Accuracy (Perceptrón): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Parte II - Implementación del Perceptrón Simple\n",
    "\n",
    "# Cargar dataset Iris (Setosa y Virginica)\n",
    "def load_iris_binary():\n",
    "    # Datos manuales para Setosa y Virginica\n",
    "    from sklearn.datasets import load_iris\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Filtrar solo Setosa (0) y Virginica (1)\n",
    "    filter_indices = (y == 0) | (y == 1)\n",
    "    return X[filter_indices], y[filter_indices]\n",
    "\n",
    "# Perceptrón Simple\n",
    "def perceptron(X, y, epochs=1000, lr=0.01):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)  # Inicializar pesos\n",
    "    bias = 0                       # Inicializar sesgo\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n_samples):\n",
    "            # Salida lineal\n",
    "            linear_output = np.dot(X[i], weights) + bias\n",
    "            # Predicción\n",
    "            y_pred = 1 if linear_output > 0 else 0\n",
    "            # Actualización de pesos y sesgo\n",
    "            update = lr * (y[i] - y_pred)\n",
    "            weights += update * X[i]\n",
    "            bias += update\n",
    "    return weights, bias\n",
    "\n",
    "# Predicción con el Perceptrón\n",
    "def perceptron_predict(X, weights, bias):\n",
    "    linear_output = np.dot(X, weights) + bias\n",
    "    return (linear_output > 0).astype(int)\n",
    "\n",
    "# Validación Hold-Out 70/30\n",
    "def hold_out_validation(X, y, test_ratio=0.3):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    split_point = int(n_samples * (1 - test_ratio))\n",
    "    train_indices, test_indices = indices[:split_point], indices[split_point:]\n",
    "    return X[train_indices], y[train_indices], X[test_indices], y[test_indices]\n",
    "\n",
    "# Programa principal\n",
    "# Cargar datos\n",
    "X, y = load_iris_binary()\n",
    "\n",
    "# Normalización de características (opcional pero recomendado)\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba (70/30)\n",
    "X_train, y_train, X_test, y_test = hold_out_validation(X, y)\n",
    "\n",
    "# Entrenar el perceptrón\n",
    "weights, bias = perceptron(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en los datos de prueba\n",
    "predictions = perceptron_predict(X_test, weights, bias)\n",
    "\n",
    "# Calcular precisión\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Hold-Out Accuracy (Perceptrón): {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
